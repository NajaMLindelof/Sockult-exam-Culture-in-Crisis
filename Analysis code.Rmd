---
title: "Culture in Crisis | SocKult exam F24"
author: "Naja MÃ¸lle Lindelof"
date: "2024"
output:
  pdf_document: default
  html_document: default
---

# Install packages and set up coding environment

LINK FOR IMPLEMENTING RENV FOR REPRODUCIBILITY:
https://raps-with-r.dev/repro_intro.html

Using the following package versions:
- Base R 4.3.2
- pacman 0.5.1
- tidyverse 2.0.0
- tidytext 0.4.2
- topicmodels 0.2.16
- tm 0.7.13
- quanteda 4.0.2
- broom 1.0.5
- scales 1.3.0
- reshape2 1.4.4
- viridis 0.6.5
- conflicted 1.2.0
- igraph 2.0.3
- bayesbio 1.0.0
- ggraph 2.2.1
- proxy 0.4.27


```{r setup, include = F}

# Setup coding environment
knitr::opts_chunk$set(echo = TRUE)

# Use pacman package to install all the other packages
pacman::p_load(
  tidyverse, 
  tidytext, 
  topicmodels, 
  tm, 
  quanteda,
  broom, 
  scales, 
  reshape2,
  viridis,
  conflicted,
  bayesbio,
  proxy,
  ggraph,
  igraph
)

```



### Load preprocessed data
See preprocessing code file for details

```{r load_preprocessed}

before <- read_csv("before_final data.csv")
during <- read_csv("during_final data.csv")
after <- read_csv("after_final data.csv")

```


```{r load_tidy_data}

tidy_before <- read_csv("before_tidy data.csv")
tidy_during <- read_csv("during_tidy data.csv")
tidy_after <- read_csv("after_tidy data.csv")
tidy_all <- read_csv("all_tidy data.csv")

```

### Plot of distribution of tweets across periods

```{r distribution histogram}

# Merge original data
all_data <- bind_rows(before, during, after)

# Make a histogram of the distribution of tweets per period
all_data %>% 
  ggplot(aes(x = Datetime, fill = Period)) +
  geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +
  facet_wrap(~ Period, ncol = 1) +
  # Implement colour blind friendly colour palette
  scale_fill_viridis_d() + 
  # Implement nice font
  theme(text = element_text(family = "Cambria")) +
  ggtitle("Distribution of tweets across periods")

```


## NLP: Topic modelling

### Convert data to DTM
For many machine learning algorithm, it is necessary to convert data to the format document-term matrix (DTM), a common matrix format representing tokens (the semantic units) as columns, documents (i.e. the tweets) as rows, and term frequency (number of times a token appears in a document) as the matrix values. As most term-document pairings will not occur at all (then the documents, i.e. tweets, would be nearly identical), many term frequency values will be 0; the amount of zero-values is the _sparsity_ of the matrix.


```{r dtm_fuction}

dtm_data <- function(tidy_data){
  "Function for converting tidy data frames containing tokens to DTMs."

  grouped_data <- tidy_data %>% 
    
    # Remove the two keywords used to scrape the data due to their naturally occuring high frequency
      # Note that I keep the third keyword, kulturpolitik, because it is meaningful for the following analysis
    dplyr::filter(word != "#dkpol", word != "#dkkultur") %>%

  # Assuming no equal datetimes in the format (Y-M-D H:M:S):
    # Reassemble the token words into their original tweets using their datetimes
    group_by(Datetime) %>% 
    mutate(Text = paste(word, collapse = " ")) %>% 
    
    # Clean up in output: Keep only one version of reassembled tweets, discard tidy words column
    ungroup() %>% 
    distinct(Datetime, .keep_all = T) %>%
    select(-word)
    
  # Make all the tweet documents into a text corpus (function returns corpus with documents in same order as input)
  corpus <- quanteda::corpus(grouped_data, text_field = "Text")

  # Convert this corpus to a DTM
  data_dtm <- tm::DocumentTermMatrix(corpus)
  
  return(data_dtm)
}

```

```{r dtm_of_tidy_data}

# Convert the data to DTMs using this custom function
dtm_before <- dtm_data(tidy_before)
dtm_during <- dtm_data(tidy_during)
dtm_after <- dtm_data(tidy_after)

```

### Fit LDA topic models using Gibbs sampling
Gibbs sampling using the default control and hyper parameters as specified in (https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf)

```{r fit_lda}

# Fit LDA topic models  
tm_before <- topicmodels::LDA(dtm_before, k = 3, method = "Gibbs", control = list(seed = 1234))
tm_during <- topicmodels::LDA(dtm_during, k = 3, method = "Gibbs", control = list(seed = 1234))
tm_after <- topicmodels::LDA(dtm_after, k = 3, method = "Gibbs", control = list(seed = 1234))

```

### Inspect topics

#### Per-topic-per-word probabilities 

```{r compute_beta}

# Compute per-topic-per-word (beta) probabilities 
topics_before <- tidytext::tidy(tm_before, matrix = "beta")
topics_during <- tidytext::tidy(tm_during, matrix = "beta")
topics_after <- tidytext::tidy(tm_after, matrix = "beta")

```


```{r top_terms}

top_terms <- function(topics_data){
  "Function for finding the most common words used in each topic in a period."

  top <- topics_data %>% 
    
    # Select the n rows of words with the highest beta-values in each topic 
    group_by(topic) %>% 
    slice_max(beta, n = 15) %>% 
    
    # Arrange these words in descending order from higher to lower beta-values for each topic
    ungroup() %>%
    arrange(topic, -beta)
  
  return(top)
}


plot_top_terms <- function(top_terms){
  "Function for plotting most common words within a topics."

  top_terms %>% 
    
    # Reorder the list of top terms by beta-values within to the topics to which the terms belong
    mutate(term = reorder_within(term, beta, topic)) %>%
    
    # Plot the top terms according to their beta-values
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    scale_y_reordered() +
  
    # Implement (a different) colour blind friendly colour palette  
    scale_fill_viridis_d(option = "C") +
    # and nice font:)
    theme(text = element_text(family = "Cambria"))

}

```


```{r}

# Calculate and plot top terms within each period, add period name
## Before
tt_before <- top_terms(topics_before)
ttplot_before <- plot_top_terms(tt_before) +
  ggtitle("Before")

## During
tt_during <- top_terms(topics_during)
ttplot_during <- plot_top_terms(tt_during) + 
  ggtitle("During")

## After
tt_after <- top_terms(topics_after)
ttplot_after <- plot_top_terms(tt_after) +
  ggtitle("After")


# Plot the figures (not with patchwork, ruins layout:()
ttplot_before
ttplot_during
ttplot_after


# Save plots using the ggsave() function
ggsave(plot = ttplot_before, file = "Before_topterms.jpeg", width = 7, height = 3.5)
ggsave(plot = ttplot_during, file = "During_topterms.jpeg", width = 7, height = 3.5)
ggsave(plot = ttplot_after, file = "After_topterms.jpeg", width = 7, height = 3.5)

```

#### Per-document-per-topic probabilities

```{r compute_gamma}

# Compute per-document-per-topic (gamma) probabilities
documents_before <- as.tibble(tm_before@gamma) %>% 
  mutate(ID = row_number())
documents_during <- as.tibble(tm_during@gamma) %>% 
  mutate(ID = row_number())
documents_after <- as.tibble(tm_after@gamma) %>% 
  mutate(ID = row_number())

```




## Network analysis


nodes = usernames
  attribute = topic distribution
edges = strength of similarity in attributes


1) aggregate topic distribution per individual
- Texts/documents in topic model = same indexing as original data

2) calculate cosine similarity 



### Compute aggregated distribution of topics per user

```{r aggregate_gamma}

gamma_dist <- function(original_data, gamma_data){
  "Function for computing/collecting topic distribution attribute for each username"
  
  # Find usernames for those who posted more than one time during the period
  duplicates <- original_data %>% 
    group_by(Username) %>%
    dplyr::filter(n() > 1) %>%
    distinct(Username) %>%
    dplyr::pull(Username)
  
  # Extract all tweet IDs for each of these usernames (i.e. multiple IDs per user)
  multi_ids <- original_data %>% 
    dplyr::filter(Username %in% duplicates) %>%
    select(ID, Username, Hashtags)
  
  # Extract topic distribution for all tweets matching these IDs (i.e. gamma probs for all tweets by more active tweeters)
  multi_topic <- gamma_data %>% 
    inner_join(multi_ids, by = "ID") %>%
    select(Username, V1, V2, V3, Hashtags)
  
  # Compute the average topic distribution for each username (i.e. average over user)
  avrg_topics <- multi_topic %>% 
    group_by(Username) %>% 
    mutate(V1 = mean(V1), V2 = mean(V2), V3 = mean(V3)) %>% 

  # Aggregate hashtags per user as well. First clean Hashtag column.
    mutate(Hashtags = gsub("\\[|\\]|'", "", Hashtags)) %>% 
    group_by(Username, V1, V2, V3) %>% 
    # Then collapse Hashtags per user while removing duplicates
    summarise(Hashtags = paste(unique(unlist(strsplit(Hashtags, ",\\s*"))), collapse = " "), .groups = 'drop')
  

  # Extract tweets from users who only tweeted once (same approach)
  singles <- original_data %>%
    dplyr::filter(!Username %in% duplicates) %>%
    select(ID, Username, Hashtags)

  singl_topics <- gamma_data %>% 
    inner_join(singles, by = "ID") %>%
    mutate(Hashtags = gsub("\\[|\\]|'", "", Hashtags)) %>% 
    select(Username, V1, V2, V3, Hashtags)


  # Get final tibble with only one (averaged) topic distribution for each username
  combined_gammas <- bind_rows(singl_topics, avrg_topics)
  
  return(combined_gammas)

}

```

```{r comp_agg_gamma}

# Aggregate gamma distributions over usernames using custom function
dist_gamma_before <- gamma_dist(before, documents_before)
dist_gamma_during <- gamma_dist(during, documents_during)
dist_gamma_after <- gamma_dist(after, documents_after)

```


### Compute pairwise cosine similarity measures of topic distributions. 

```{r cosim_func}

cosim <- function(gamma_dist){
  "Function for computing pairwise cosine similarity measures using proxy"
  
  # Convert gamma distribution file to matrix format
  matrix_dist <- gamma_dist %>%
    select(-Hashtags) %>% 
    column_to_rownames("Username") %>%
    as.matrix()
  
  # Calculate the cosine similarity between each pair of persons
  cosine_sim <- proxy::simil(matrix_dist, method = "cosine")

  # Convert matrix of cosine similarity to tibble
  cosine_sim <- as.data.frame(as.table(as.matrix(cosine_sim)))
  colnames(cosine_sim) <- c("from", "to", "cosine_similarity")

  cosine_sim <- cosine_sim %>% 
    as_tibble() %>% 
    
    # Discard NA similarity measures that are between the same user
    dplyr::filter(!is.na(cosine_similarity))
  
  # Convert data formats
  #cosine_sim$from <- as.character(cosine_sim$from)
  #cosine_sim$to <- as.character(cosine_sim$to)
    
  return(cosine_sim)
  
}

```


```{r compute_cosim}

# Compute cosine similarity measures
before_co_sim <- cosim(before_dist_gamma)
during_co_sim <- cosim(during_dist_gamma)
after_co_sim <- cosim(after_dist_gamma)

```

### Compute pairwise Jaccard similarity measures of hashtags

```{r jacsim_func}

jac_sim <- function(gamma_dist){
  "Function for computing jaccard similarity measures between hashtags using package bayesbio"
  
  # Compute sets for each string of hashtags per user
  gamma_dist <- gamma_dist %>%
  mutate(HashtagsSet = strsplit(Hashtags, ",\\s*"))

  # Build empty matrix of similarity measures
  nu <- length(gamma_dist$Username)
  similarity_matrix <- matrix(0, nrow = nu, ncol = nu,
                              dimnames = list(gamma_dist$Username, gamma_dist$Username))
  
  # For loop for calculating Jaccard similarity pairwise among all users 
  for (i in 1:(nu - 1)) {
    for (j in (i + 1):nu) {
      
      # Define sets of hashtags
      set1 <- gamma_dist$HashtagsSet[[i]]
      set2 <- gamma_dist$HashtagsSet[[j]]
      
      # Compute Jaccard similarity using bayesbio package
      similarity <- bayesbio::jaccardSets(set1, set2)
      
      # Save similarity measure to matrix
      similarity_matrix[i, j] <- similarity
      similarity_matrix[j, i] <- similarity
    }
  }
  
  # Convert matrix of jaccard similarity to tibble
  jaccard_sim <- as.data.frame(as.table(similarity_matrix))
  colnames(jaccard_sim) <- c("from", "to", "jaccard_similarity")

  jaccard_sim <- jaccard_sim %>% 
    as_tibble() %>% 
  
    # Discard similarity measures that are between the same user
    dplyr::filter(!from == to)

  return(jaccard_sim)
  
}

```

```{r compute_jacsim}

# Compute cosine similarity measures
before_jac_sim <- jac_sim(before_dist_gamma)
during_jac_sim <- jac_sim(during_dist_gamma)
after_jac_sim <- jac_sim(after_dist_gamma)

```

### Build network

```{r build_net}

network <- function(jaccard_sim){
  "Function for building network using igraph"
  
  graph_net <- jaccard_sim %>% 
    # Ties with existing nonzero similarity only 
    dplyr::filter(jaccard_similarity > 0) %>% 
    
    # Undirected network
    igraph::graph_from_data_frame(directed = F)
  
  return(graph_net)
  
}

```


```{r compute_nets}

# Build networks using custom function network()
before_graph <- network(before_jac_sim)
during_graph <- network(during_jac_sim)
after_graph <- network(after_jac_sim)

```

#### Compute network metrics

**Nodal and dyadic level:**
- degrees aka degree centrality: number of edges a given node has
- path length (or path distance): the number of lines in the sequence from the beginning node to the end node

- centrality: 
1) Betweenness centrality: the extent to which node i is on the
geodesic paths of all other pairs of nodes in a network
2) Closenss centrality: how fast node i can reach all other
nodes in a network - closeness centrality emphasizes speed of
connections through both direct and indirect ties to all network nodes (comparing nodes within network; sensitive to network size) --> 
3) Eigen centrality, special instance of degree centrality, weights each
of the nodes with direct ties to an actor by their centralities --> high values = high prestige + cnnections with other high value nodes

**Subgroup level**
- component aka component subgraphs: portions of the network that are disconnected from each other
- clustering coefficient aka transitivity: probability that the adjacent nodes of a node are connected --> extent to wich nodes cluster together


**Network level:**
- network size: Number of nodes and edges --> network magnitude
- density of network: indication of the extent to which the number of realised ties approach the maximum possible number ties --> connectivity measure
- link density: average number of links per node --> connectivity measure
- centralisation: network level measure of centralisation based on node level centrality scores



```{r network_metrics}

net_metrics <- function(graph_net){
  "Function for computing and showing metrics at both nodal, dyadic/triadic, and whole network levels in network analysis."
  
  ## Nodal and dyadic levels - applied to whole network
  
  # Degrees aka degree centrality
  degree <- degree(graph_net)
  max_deg <- max(degree) 
  min_deg <- min(degree)
  mean_deg <- mean(degree)
  
  # Path length
  mean_path <- mean_distance(graph_net)
  longest_path <- diameter(graph_net)
  
  # Centrality measures (not included)
  # Betweenness centrality
  betweenness_vals <- betweenness(graph_net)
  # Closeness centrality
  closeness_vals <- closeness(graph_net)
  # Eigenvector centrality
  eigenvector_vals <- eigen_centrality(graph_net)$vector

  
  ## Subgroups level
  
  # Count subgroups aka cliques (not included)
  #cliques <- count_max_cliques(graph_net)
  
  # Transitivity aka clustering coeff
  clustering_coeff <- transitivity(graph_net, type = "global")
  
  # Components
  components <- count_components(graph_net)
  
  ## Network level metrics
  
  # Network size
  size <- vcount(graph_net)
  edges <- ecount(graph_net)
  link_dens <- edges/size
  
  # Density
  density <- edge_density(graph_net)
  
  # Centralisation
  centralisation <- centr_degree(graph_net)$centralization
  
  # Subgraphs/cliques
  
  ## Combine metrics
  metrics = tibble(
    Nodes = size,
    Edges = edges,
    "Link density" = link_dens,
    Density = density,
    Centralisation = centralisation,
    Transitivity = clustering_coeff,
    Components = components,
    #Cliques = cliques,
    "Mean path length" = mean_path,
    "Longest path" = longest_path,
    "Mean degree centrality" = mean_deg,
    "Max degree centrality" = max_deg,
    "Min degree centrality" = min_deg
  )
  
  return(metrics)
  
}

nodal_net_metrics <- function(graph_net, gamma_dist){
  "Function for computing and showing degree centrality at nodal level including attribute 'Hashtag'."
  
  degree <- degree(graph_net)
  
  metrics <- tibble(
    Node = names(degree),
    "Degree centrality" = degree  
  )
  
  metrics_full <- metrics %>% 
    rename(Username = Node) %>% 
    right_join(gamma_dist, by = "Username") %>% 
    select(-V1, -V2, -V3)
  
  return(metrics_full)
  
}

```


```{r compute_metrics}

# Compute metrics
before_metrics <- net_metrics(before_graph)
during_metrics <- net_metrics(during_graph)
after_metrics <- net_metrics(after_graph)


# Join metrics
joined_m <- before_metrics %>% 
  left_join(during_metrics)
  
joined_m <- full_join(before_metrics, during_metrics)
joined_m <- full_join(joined_m, after_metrics)

joined_m <- joined_m %>% 
  mutate(Period = c("Before", "During", "After")) %>% 
  select(Period, everything())

```


```{r net_plot}

net_plot <- function(graph_net){
  "Function for plotting networks nicely using ggraph."
  
  graph_net %>% 
    ggraph::ggraph(layout = "fr") +
    
    # Colour and give weight to edges using similarity measure
    geom_edge_link(aes(
      width = jaccard_similarity, 
      colour = jaccard_similarity)) +
   
    geom_node_point() +
    scale_edge_width(range = c(0.5, 2)) +
    scale_edge_color_gradient(low = "blue", high = "red") +
    theme_void()
    
}


main_comp_plot <- function(graph_net){
  "Function for calculating and plotting the largest component of a network."
  
  # Find all components
  components <- components(graph_net)
  
  # Find largest component, i.e. component with highest number of nodes
  largest_component <- which.max(components$csize)
  
  # Find usernames that are part of this largest components
  subgraph_nodes <- igraph::V(graph_net)[
    components$membership == largest_component]

  # Calculate subgraph from these nodes and their edges
  subgraph <- igraph::induced_subgraph(graph_net, subgraph_nodes)
  
  # Plot using custom network plot function
  net_plot(subgraph)
  
}


```



```{r plot_netws}

# Plot networks
before_plot <- net_plot(before_graph)
during_plot <- net_plot(during_graph)
after_plot <- net_plot(after_graph)

# Calculate and plot main component
before_main_component_plot <- main_comp_plot(before_graph)
during_main_component_plot <- main_comp_plot(during_graph)
after_main_component_plot <- main_comp_plot(after_graph)

```


```{r save_net_plot}

# Save plots using the ggsave function
ggsave(plot = after_main_component_plot, file = "after_main_component_plot.jpeg", width = 7, height = 3.5)

```














