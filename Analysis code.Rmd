---
title: "Culture in Crisis | SocKult exam F24"
author: "Naja MÃ¸lle Lindelof"
date: "2024"
output:
  pdf_document: default
  html_document: default
---

# Install packages and set up coding environment

LINK FOR IMPLEMENTING RENV FOR REPRODUCIBILITY:
https://raps-with-r.dev/repro_intro.html

Using the following package versions:
- Base R 4.3.2
- pacman 0.5.1
- tidyverse 2.0.0
- tidytext 0.4.2
- topicmodels 0.2.16
- tm 0.7.13
- quanteda 4.0.2
- broom 1.0.5
- scales 1.3.0
- reshape2 1.4.4
- viridis 0.6.5
- conflicted 1.2.0


```{r setup, include = F}

# Setup coding environment
knitr::opts_chunk$set(echo = TRUE)

# Use pacman package to install all the other packages
pacman::p_load(
  tidyverse, 
  tidytext, 
  topicmodels, 
  tm, 
  quanteda,
  broom, 
  scales, 
  reshape2,
  viridis,
  conflicted
)

```



### Load preprocessed data
See preprocessing code file for details

```{r load_preprocessed}

before <- read_csv("before_final data.csv")
during <- read_csv("during_final data.csv")
after <- read_csv("after_final data.csv")

```


```{r load_tidy_data}

tidy_before <- read_csv("before_tidy data.csv")
tidy_during <- read_csv("during_tidy data.csv")
tidy_after <- read_csv("after_tidy data.csv")
tidy_all <- read_csv("all_tidy data.csv")

```

### Plot of distribution of tweets across periods

```{r distribution histogram}

# Merge original data
all_data <- bind_rows(before, during, after)

# Make a histogram of the distribution of tweets per period
all_data %>% 
  ggplot(aes(x = Datetime, fill = Period)) +
  geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +
  facet_wrap(~ Period, ncol = 1) +
  # Implement colour blind friendly colour palette
  scale_fill_viridis_d() + 
  # Implement nice font
  theme(text = element_text(family = "Cambria")) +
  ggtitle("Distribution of tweets across periods")

```


## NLP: Topic modelling

### Convert data to DTM
For many machine learning algorithm, it is necessary to convert data to the format document-term matrix (DTM), a common matrix format representing tokens (the semantic units) as columns, documents (i.e. the tweets) as rows, and term frequency (number of times a token appears in a document) as the matrix values. As most term-document pairings will not occur at all (then the documents, i.e. tweets, would be nearly identical), many term frequency values will be 0; the amount of zero-values is the _sparsity_ of the matrix.


```{r dtm_fuction}

# Build function for converting the tidy data frames containing the tokens to DTMs
dtm_data <- function(tidy_data){
  grouped_data <- tidy_data %>% 
    
    # Remove the two keywords used to scrape the data due to their naturally occuring high frequency
      # Note that I keep the third keyword, kulturpolitik, because it is meaningful for the following analysis
    dplyr::filter(word != "#dkpol", word != "#dkkultur") %>%

  # Assuming no equal datetimes in the format (Y-M-D H:M:S):
    # Reassemble the token words into their original tweets using their datetimes
    group_by(Datetime) %>% 
    mutate(Text = paste(word, collapse = " ")) %>% 
    
    # Clean up in output: Keep only one version of reassembled tweets, discard tidy words column
    ungroup() %>% 
    distinct(Datetime, .keep_all = T) %>%
    select(-word)
    
  # Make all the tweet documents into a text corpus (function returns corpus with documents in same order as input)
  corpus <- quanteda::corpus(grouped_data, text_field = "Text")

  # Convert this corpus to a DTM
  data_dtm <- tm::DocumentTermMatrix(corpus)
  
  return(data_dtm)
}

```

```{r dtm_of_tidy_data}

# Convert the data to DTMs using this custom function
dtm_before <- dtm_data(tidy_before)
dtm_during <- dtm_data(tidy_during)
dtm_after <- dtm_data(tidy_after)

```

### Fit LDA topic models using Gibbs sampling
Gibbs sampling using the default control and hyper parameters as specified in (https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf)

```{r fit_lda}

# Fit LDA topic models  
tm_before <- topicmodels::LDA(dtm_before, k = 3, method = "Gibbs", control = list(seed = 1234))
tm_during <- topicmodels::LDA(dtm_during, k = 3, method = "Gibbs", control = list(seed = 1234))
tm_after <- topicmodels::LDA(dtm_after, k = 3, method = "Gibbs", control = list(seed = 1234))

```

### Inspect topics

#### Per-topic-per-word probabilities 

```{r compute_beta}

# Compute per-topic-per-word (beta) probabilities 
topics_before <- tidytext::tidy(tm_before, matrix = "beta")
topics_during <- tidytext::tidy(tm_during, matrix = "beta")
topics_after <- tidytext::tidy(tm_after, matrix = "beta")

```


```{r top_terms}

# Build function for finding the most common words used in each topic in a period
top_terms <- function(topics_data){
  top <- topics_data %>% 
    
    # Select the n rows of words with the highest beta-values in each topic 
    group_by(topic) %>% 
    slice_max(beta, n = 15) %>% 
    
    # Arrange these words in descending order from higher to lower beta-values for each topic
    ungroup() %>%
    arrange(topic, -beta)
  
  return(top)
}


# Build function for plotting these 
plot_top_terms <- function(top_terms){
  top_terms %>% 
    
    # Reorder the list of top terms by beta-values within to the topics to which the terms belong
    mutate(term = reorder_within(term, beta, topic)) %>%
    
    # Plot the top terms according to their beta-values
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    scale_y_reordered() +
  
    # Implement (a different) colour blind friendly colour palette  
    scale_fill_viridis_d(option = "C") +
    # and nice font:)
    theme(text = element_text(family = "Cambria"))

}

```


```{r}

# Calculate and plot top terms within each period, add period name
## Before
tt_before <- top_terms(topics_before)
ttplot_before <- plot_top_terms(tt_before) +
  ggtitle("Before")

## During
tt_during <- top_terms(topics_during)
ttplot_during <- plot_top_terms(tt_during) + 
  ggtitle("During")

## After
tt_after <- top_terms(topics_after)
ttplot_after <- plot_top_terms(tt_after) +
  ggtitle("After")


# Plot the figures (not with patchwork, ruins layout:()
ttplot_before
ttplot_during
ttplot_after


# Save plots using the ggsave() function
ggsave(plot = ttplot_before, file = "Before_topterms.jpeg", width = 7, height = 3.5)
ggsave(plot = ttplot_during, file = "During_topterms.jpeg", width = 7, height = 3.5)
ggsave(plot = ttplot_after, file = "After_topterms.jpeg", width = 7, height = 3.5)

```

#### Per-document-per-topic probabilities

```{r compute_gamma}

# Compute per-document-per-topic (gamma) probabilities
documents_before <- as.tibble(tm_before@gamma) %>% 
  mutate(ID = row_number())
documents_during <- as.tibble(tm_during@gamma) %>% 
  mutate(ID = row_number())
documents_after <- as.tibble(tm_after@gamma) %>% 
  mutate(ID = row_number())

```




## Network analysis


nodes = usernames
  attribute = topic distribution
edges = strength of similarity in attributes


1) aggregate topic distribution per individual
- Texts/documents in topic model = same indexing as original data

2) calculate cosine similarity 



### Compute aggregated distribution of topics per user

```{r aggregate_gamma}

# Build function for computing/collecting topic distribution attribute for each username
gamma_dist <- function(original_data, gamma_data){
  
  # Find usernames for those who posted more than one time during the period
  duplicates <- original_data %>% 
    group_by(Username) %>%
    dplyr::filter(n() > 1) %>%
    distinct(Username) %>%
    dplyr::pull(Username)
  
  # Extract all tweet IDs for each of these usernames (i.e. multiple IDs per user)
  multi_ids <- original_data %>% 
    dplyr::filter(Username %in% duplicates) %>%
    select(ID, Username, Hashtags)
  
  # Extract topic distribution for all tweets matching these IDs (i.e. gamma probs for all tweets by more active tweeters)
  multi_topic <- gamma_data %>% 
    inner_join(multi_ids, by = "ID") %>%
    select(Username, V1, V2, V3, Hashtags)
  
  # Compute the average topic distribution for each username (i.e. average over user)
  avrg_topics <- multi_topic %>% 
    group_by(Username) %>% 
    mutate(V1 = mean(V1), V2 = mean(V2), V3 = mean(V3)) %>% 

  # Aggregate hashtags per user as well. First clean Hashtag column.
    mutate(Hashtags = gsub("\\[|\\]|'", "", Hashtags)) %>% 
    group_by(Username, V1, V2, V3) %>% 
    # Then collapse Hashtags per user while removing duplicates
    summarise(Hashtags = paste(unique(unlist(strsplit(Hashtags, ",\\s*"))), collapse = " "), .groups = 'drop')
  

  # Extract tweets from users who only tweeted once (same approach)
  singles <- original_data %>%
    dplyr::filter(!Username %in% duplicates) %>%
    select(ID, Username, Hashtags)

  singl_topics <- gamma_data %>% 
    inner_join(singles, by = "ID") %>%
    mutate(Hashtags = gsub("\\[|\\]|'", "", Hashtags)) %>% 
    select(Username, V1, V2, V3, Hashtags)


  # Get final tibble with only one (averaged) topic distribution for each username
  combined_gammas <- bind_rows(singl_topics, avrg_topics)
  
  return(combined_gammas)

}

```

```{r comp_agg_gamma}

# Aggregate gamma distributions over usernames using custom function
dist_gamma_before <- gamma_dist(before, documents_before)
dist_gamma_during <- gamma_dist(during, documents_during)
dist_gamma_after <- gamma_dist(after, documents_after)

```


### Compute pairwise cosine similarity measures of topic distributions. 

```{r cosim_func}

# Build function for computing cosine similarity measures using package proxy
cosim <- function(gamma_dist){
  
  # Convert gamma distribution file to matrix format
  matrix_dist <- gamma_dist %>%
    select(-Hashtags) %>% 
    column_to_rownames("Username") %>%
    as.matrix()
  
  # Calculate the cosine similarity between each pair of persons
  cosine_sim <- proxy::simil(matrix_dist, method = "cosine")

  # Convert matrix of cosine similarity to tibble
  cosine_sim <- as.data.frame(as.table(as.matrix(cosine_sim)))
  colnames(cosine_sim) <- c("from", "to", "cosine_similarity")

  cosine_sim <- cosine_sim %>% 
    as_tibble() %>% 
    
    # Discard NA similarity measures that are between the same user
    dplyr::filter(!is.na(cosine_similarity))
  
  # Convert data formats
  #cosine_sim$from <- as.character(cosine_sim$from)
  #cosine_sim$to <- as.character(cosine_sim$to)
    
  return(cosine_sim)
  
}

```


```{r compute_cosim}

# Compute cosine similarity measures
before_co_sim <- cosim(before_dist_gamma)
during_co_sim <- cosim(during_dist_gamma)
after_co_sim <- cosim(after_dist_gamma)

```

### Compute pairwise Jaccard similarity measures of hashtags

```{r jacsim_func}

# Build function for computing jaccard similarity measures between hashtags using package bayesbio
jac_sim <- function(gamma_dist){
  
  # Compute sets for each string of hashtags per user
  gamma_dist <- gamma_dist %>%
  mutate(HashtagsSet = strsplit(Hashtags, ",\\s*"))

  # Build empty matrix of similarity measures
  users <- gamma_dist$Username
  nu <- length(users)
  similarity_matrix <- matrix(0, nrow = nu, ncol = nu,
                              dimnames = list(users, users))
  
  # For loop for calculating Jaccard similarity pairwise among all users 
  for (i in 1:(nu - 1)) {
    for (j in (i + 1):nu) {
      
      # Define sets of hashtags
      set1 <- gamma_dist$HashtagsSet[[i]]
      set2 <- gamma_dist$HashtagsSet[[j]]
      
      # Compute Jaccard similarity using bayesbio package
      similarity <- bayesbio::jaccardSets(set1, set2)
      
      # Save similarity measure to matrix
      similarity_matrix[i, j] <- similarity
      similarity_matrix[j, i] <- similarity
    }
  }
  
  # Convert matrix of jaccard similarity to tibble
  jaccard_sim <- as.data.frame(as.table(similarity_matrix))
  colnames(jaccard_sim) <- c("from", "to", "jaccard_similarity")

  jaccard_sim <- jaccard_sim %>% 
    as_tibble() %>% 
  
    # Discard similarity measures that are between the same user
    dplyr::filter(!from == to)

  return(jaccard_sim)
  
}

```

```{r compute_jacsim}

# Compute cosine similarity measures
before_jac_sim <- jac_sim(before_dist_gamma)
during_jac_sim <- jac_sim(during_dist_gamma)
after_jac_sim <- jac_sim(after_dist_gamma)

```


```{r save_sim}

# Save both similarity measures locally
write.csv(before_co_sim, "before_cosine.csv", row.names = FALSE)
write.csv(during_co_sim, "during_cosine.csv", row.names = FALSE)
write.csv(after_co_sim, "after_cosine.csv", row.names = FALSE)

write.csv(before_jac_sim, "before_jaccard.csv", row.names = FALSE)
write.csv(during_jac_sim, "during_jaccard.csv", row.names = FALSE)
write.csv(after_jac_sim, "after_jaccard.csv", row.names = FALSE)

```














